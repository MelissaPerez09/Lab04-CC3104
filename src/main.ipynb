{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c476ae4",
   "metadata": {},
   "source": [
    "## Laboratorio 04 - Métodos de Monte Carlo\n",
    "Integrantes:\n",
    "- Ricardo Méndez\n",
    "- Melissa Pérez\n",
    "\n",
    "Repositorio: https://github.com/MelissaPerez09/Lab04-CC3104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c238c",
   "metadata": {},
   "source": [
    "### Task 01\n",
    "1. ¿Cómo afecta la elección de la estrategia de exploración (exploring starts vs soft policy) a la precisión de la evaluación de políticas en los métodos de Monte Carlo?\n",
    "    \n",
    "    a. Considere la posibilidad de comparar el desempeño de las políticas evaluadas con y sin explorar los inicios o con diferentes niveles de exploración en políticas blandas.\n",
    "    - De manera general, Exploring Starts ofrece evaluaciones más precisas pero necesita un control del entorno, mientras que las soft policies ofrecen una solución práctica y más aplicable, aunque con menor cobertura teórica.\n",
    "    - Al usar Exploring Starts, se fuerza al agente a comenzar desde cualquier estado y acción al azar. Esto ayuda mucho porque garantiza que explore todo el espacio posible, lo que hace que la evaluación de la política sea más precisa.\n",
    "    - Una soft policy, como una política ε-greedy, permite al agente elegir la mejor acción la mayoría del tiempo, pero de vez en cuando prueba otras acciones. Mantiene la exploración constante mientras aprende.\n",
    "\n",
    "2. En el contexto del aprendizaje de Monte Carlo fuera de la póliza, ¿cómo afecta la razón de muestreo de importancia a la convergencia de la evaluación de políticas? Explore cómo la razón de muestreo de importancia afecta la estabilidad y la convergencia.\n",
    "    - En un aprendizaje off-policy, se aprender sobre una política (objetivo) usando datos generados por otra (la de comportamiento). Para ajustar esto, se utiliza muestreo de importancia, que es una razón que indica qué tanto se debe corregir lo aprendido para que refleje la política objetivo.\n",
    "\n",
    "3. ¿Cómo puede el uso de una soft policy influir en la eficacia del aprendizaje de políticas óptimas en comparación con las políticas deterministas en los métodos de Monte Carlo? Compare el desempeño y los resultados de aprendizaje de las políticas derivadas de estrategias épsilon-greedy con las derivadas de políticas deterministas.\n",
    "    - Una soft policy como ε-greedy sigue explorando aunque ya tenga buenas acciones, porque siempre deja un espacio pequeño para probar nuevas cosas. Esto es bueno porque ayuda a descubrir mejores soluciones que quizá no se habían considerado.\n",
    "    - En cambio, una política determinista elige siempre la mejor acción conocida. Es más rápida en tomar decisiones, pero si esa “mejor acción” no es realmente la óptima global, se puede quedar atascada.\n",
    "\n",
    "4. ¿Cuáles son los posibles beneficios y desventajas de utilizar métodos de Monte Carlo off-policy en comparación con los on-policy en términos de eficiencia de la muestra, costo computacional y velocidad de aprendizaje?\n",
    "    - Los principales beneficios a considerar es que los algoritmos off-policy ofrecen una eficiencia de muestra porque puede usar cualquier experiencia recolectada, el costo computacional es más alto por el uso de muestreo de importancia, la velocidad de aprendizaje depende de si las políticas no son muy distintas. Por último, mencionar que su principal desventaja es que es más sensible y difícil de ajustar.\n",
    "    - Por otro lado, con los algoritmos on-policy se tiene el caso contrario a lo mencionado con anterioridad, la eficiencia de muestra usa únicamente la que genera bajo su propia política, el costo computacional es más bajo porque los cálculos son más directos y la velocidad de aprendizaje es más estable pero depende de la exploración. Su principal desventaja es que es menos eficiente con los datos porque deben ser controlados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce35661c",
   "metadata": {},
   "source": [
    "### Task 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed2aa9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdde386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definicion del entorno\n",
    "class InventoryEnvironment:\n",
    "    def __init__(self):\n",
    "        self.products = ['product_A', 'product_B']\n",
    "        self.max_stock = 10 # Pueden cambiar este número si gustan\n",
    "        self.demand = {'product_A': [0, 1, 2], 'product_B': [0, 1, 2]}\n",
    "        self.restock_cost = {'product_A': 5, 'product_B': 7}\n",
    "        self.sell_price = {'product_A': 10, 'product_B': 15}\n",
    "        self.state = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = {product: random.randint(0, self.max_stock) for product in self.products}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for product in self.products:\n",
    "            stock = self.state[product]\n",
    "            restock = action[product]\n",
    "            self.state[product] = min(self.max_stock, stock + restock)\n",
    "            demand = random.choice(self.demand[product])\n",
    "            sales = min(demand, self.state[product])\n",
    "            self.state[product] -= sales\n",
    "            reward += sales * self.sell_price[product] - restock * self.restock_cost[product]\n",
    "        return self.state, reward\n",
    "    \n",
    "# Init el ambiente\n",
    "env = InventoryEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ac819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generar episodios\n",
    "def generate_episode(policy, days=30, start_state=None, start_action=None):\n",
    "    episode = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    if start_state:\n",
    "        env.state = start_state.copy()\n",
    "        state = env.state\n",
    "\n",
    "    for day in range(days):\n",
    "        if day == 0 and start_action:\n",
    "            action = start_action\n",
    "        else:\n",
    "            action = policy(state)\n",
    "        \n",
    "        next_state, reward = env.step(action)\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24cc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring starts\n",
    "def random_action():\n",
    "    return {\n",
    "        'product_A': random.randint(0, env.max_stock),\n",
    "        'product_B': random.randint(0, env.max_stock)\n",
    "    }\n",
    "\n",
    "def exploring_starts(num_episodes=500):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        start_state = {\n",
    "            'product_A': random.randint(0, env.max_stock),\n",
    "            'product_B': random.randint(0, env.max_stock)\n",
    "        }\n",
    "        start_action = random_action()\n",
    "        episode = generate_episode(policy=random_policy, start_state=start_state, start_action=start_action)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05175b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft policy\n",
    "epsilon = 0.1\n",
    "\n",
    "def random_policy(state):\n",
    "    return {\n",
    "        'product_A': random.randint(0, 2),\n",
    "        'product_B': random.randint(0, 2)\n",
    "    }\n",
    "\n",
    "def greedy_policy(Q, state):\n",
    "    state_key = (state['product_A'], state['product_B'])\n",
    "    if state_key in Q:\n",
    "        return max(Q[state_key], key=Q[state_key].get)\n",
    "    else:\n",
    "        return random_policy(state)\n",
    "\n",
    "def epsilon_greedy_policy(Q, state):\n",
    "    if random.random() < epsilon:\n",
    "        return random_policy(state)\n",
    "    else:\n",
    "        return greedy_policy(Q, state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c733a91",
   "metadata": {},
   "source": [
    "### Preguntas\n",
    "1. ¿Cuál es el valor estimado de mantener diferentes niveles de existencias para cada producto?\n",
    "2. ¿Cómo afecta el valor epsilon en la política blanda al rendimiento?\n",
    "3. ¿Cuál es el impacto de utilizar el aprendizaje fuera de la política en comparación con el aprendizaje dentro\n",
    "de la política?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
