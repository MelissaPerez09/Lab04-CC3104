{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c476ae4",
   "metadata": {},
   "source": [
    "## Laboratorio 04 - Métodos de Monte Carlo\n",
    "Integrantes:\n",
    "- Ricardo Méndez\n",
    "- Melissa Pérez\n",
    "\n",
    "Repositorio: https://github.com/MelissaPerez09/Lab04-CC3104"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083c238c",
   "metadata": {},
   "source": [
    "### Task 01\n",
    "1. ¿Cómo afecta la elección de la estrategia de exploración (exploring starts vs soft policy) a la precisión de la evaluación de políticas en los métodos de Monte Carlo?\n",
    "    \n",
    "    a. Considere la posibilidad de comparar el desempeño de las políticas evaluadas con y sin explorar los inicios o con diferentes niveles de exploración en políticas blandas.\n",
    "    - De manera general, Exploring Starts ofrece evaluaciones más precisas pero necesita un control del entorno, mientras que las soft policies ofrecen una solución práctica y más aplicable, aunque con menor cobertura teórica.\n",
    "    - Al usar Exploring Starts, se fuerza al agente a comenzar desde cualquier estado y acción al azar. Esto ayuda mucho porque garantiza que explore todo el espacio posible, lo que hace que la evaluación de la política sea más precisa.\n",
    "    - Una soft policy, como una política ε-greedy, permite al agente elegir la mejor acción la mayoría del tiempo, pero de vez en cuando prueba otras acciones. Mantiene la exploración constante mientras aprende.\n",
    "\n",
    "2. En el contexto del aprendizaje de Monte Carlo fuera de la póliza, ¿cómo afecta la razón de muestreo de importancia a la convergencia de la evaluación de políticas? Explore cómo la razón de muestreo de importancia afecta la estabilidad y la convergencia.\n",
    "    - En un aprendizaje off-policy, se aprender sobre una política (objetivo) usando datos generados por otra (la de comportamiento). Para ajustar esto, se utiliza muestreo de importancia, que es una razón que indica qué tanto se debe corregir lo aprendido para que refleje la política objetivo.\n",
    "\n",
    "3. ¿Cómo puede el uso de una soft policy influir en la eficacia del aprendizaje de políticas óptimas en comparación con las políticas deterministas en los métodos de Monte Carlo? Compare el desempeño y los resultados de aprendizaje de las políticas derivadas de estrategias épsilon-greedy con las derivadas de políticas deterministas.\n",
    "    - Una soft policy como ε-greedy sigue explorando aunque ya tenga buenas acciones, porque siempre deja un espacio pequeño para probar nuevas cosas. Esto es bueno porque ayuda a descubrir mejores soluciones que quizá no se habían considerado.\n",
    "    - En cambio, una política determinista elige siempre la mejor acción conocida. Es más rápida en tomar decisiones, pero si esa “mejor acción” no es realmente la óptima global, se puede quedar atascada.\n",
    "\n",
    "4. ¿Cuáles son los posibles beneficios y desventajas de utilizar métodos de Monte Carlo off-policy en comparación con los on-policy en términos de eficiencia de la muestra, costo computacional y velocidad de aprendizaje?\n",
    "    - Los principales beneficios a considerar es que los algoritmos off-policy ofrecen una eficiencia de muestra porque puede usar cualquier experiencia recolectada, el costo computacional es más alto por el uso de muestreo de importancia, la velocidad de aprendizaje depende de si las políticas no son muy distintas. Por último, mencionar que su principal desventaja es que es más sensible y difícil de ajustar.\n",
    "    - Por otro lado, con los algoritmos on-policy se tiene el caso contrario a lo mencionado con anterioridad, la eficiencia de muestra usa únicamente la que genera bajo su propia política, el costo computacional es más bajo porque los cálculos son más directos y la velocidad de aprendizaje es más estable pero depende de la exploración. Su principal desventaja es que es menos eficiente con los datos porque deben ser controlados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce35661c",
   "metadata": {},
   "source": [
    "### Task 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "ed2aa9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "5fdde386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definicion del entorno\n",
    "class InventoryEnvironment:\n",
    "    def __init__(self):\n",
    "        self.products = ['product_A', 'product_B']\n",
    "        self.max_stock = 10 # Pueden cambiar este número si gustan\n",
    "        self.demand = {'product_A': [0, 1, 2], 'product_B': [0, 1, 2]}\n",
    "        self.restock_cost = {'product_A': 5, 'product_B': 7}\n",
    "        self.sell_price = {'product_A': 10, 'product_B': 15}\n",
    "        self.state = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = {product: random.randint(0, self.max_stock) for product in self.products}\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        for product in self.products:\n",
    "            stock = self.state[product]\n",
    "            restock = action[product]\n",
    "            self.state[product] = min(self.max_stock, stock + restock)\n",
    "            demand = random.choice(self.demand[product])\n",
    "            sales = min(demand, self.state[product])\n",
    "            self.state[product] -= sales\n",
    "            reward += sales * self.sell_price[product] - restock * self.restock_cost[product]\n",
    "        return self.state, reward\n",
    "    \n",
    "# Init el ambiente\n",
    "env = InventoryEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "c0ac819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generar episodios\n",
    "def generate_episode(policy, days=30, start_state=None, start_action=None):\n",
    "    episode = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    if start_state:\n",
    "        env.state = start_state.copy()\n",
    "        state = env.state\n",
    "\n",
    "    for day in range(days):\n",
    "        if day == 0 and start_action:\n",
    "            action = start_action\n",
    "        else:\n",
    "            action = policy(state)\n",
    "        \n",
    "        if isinstance(action, tuple):\n",
    "            action = policy(state)\n",
    "            pass\n",
    "\n",
    "        next_state, reward = env.step(action)\n",
    "        episode.append((state.copy(), action.copy(), reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e24cc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring starts\n",
    "def random_action():\n",
    "    return {\n",
    "        'product_A': random.randint(0, env.max_stock),\n",
    "        'product_B': random.randint(0, env.max_stock)\n",
    "    }\n",
    "\n",
    "def random_policy(state):\n",
    "    return {\n",
    "        'product_A': random.randint(0, 2),\n",
    "        'product_B': random.randint(0, 2)\n",
    "    }\n",
    "\n",
    "def exploring_starts(num_episodes=500):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        start_state = {\n",
    "            'product_A': random.randint(0, env.max_stock),\n",
    "            'product_B': random.randint(0, env.max_stock)\n",
    "        }\n",
    "        start_action = random_action()\n",
    "        episode = generate_episode(policy=random_policy, start_state=start_state, start_action=start_action)\n",
    "        episodes.append(episode)\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05175b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft policy\n",
    "epsilon = 0.1\n",
    "\n",
    "def greedy_policy(Q, state):\n",
    "    state_key = (state['product_A'], state['product_B'])\n",
    "    if state_key in Q:\n",
    "        max_action = max(Q[state_key], key=Q[state_key].get)\n",
    "        return {'product_A': max_action[0], 'product_B': max_action[1]}\n",
    "    else:\n",
    "        return random_policy(state)\n",
    "\n",
    "def epsilon_greedy_policy(Q, epsilon):\n",
    "    epsilon = epsilon\n",
    "    def policy(state):\n",
    "        if random.random() < epsilon:\n",
    "            return random_policy(state)\n",
    "        else:\n",
    "            return greedy_policy(Q, state)\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "5dd044f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_mc_control(num_episodes=500, days=30, epsilon=0.1):\n",
    "    Q = {}\n",
    "    returns_count = {}\n",
    "    rewards_per_day = [0] *  days\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        policy = epsilon_greedy_policy(Q, epsilon)  # generar la política con Q actualizado\n",
    "        episode = generate_episode(policy=policy, days=days)\n",
    "        \n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G += reward\n",
    "            rewards_per_day[t] += reward\n",
    "            state_key = (state['product_A'], state['product_B'])\n",
    "            action_key = (action['product_A'], action['product_B'])\n",
    "            \n",
    "            if state_key not in Q:\n",
    "                Q[state_key] = {}\n",
    "            if action_key not in Q[state_key]:\n",
    "                Q[state_key][action_key] = 0\n",
    "                returns_count[(state_key, action_key)] = 0\n",
    "            \n",
    "            returns_count[(state_key, action_key)] += 1\n",
    "            alpha = 1 / returns_count[(state_key, action_key)]\n",
    "            Q[state_key][action_key] += alpha * (G - Q[state_key][action_key])\n",
    "    \n",
    "    return Q, rewards_per_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "40baa19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per day (exploring starts): 317.41\n"
     ]
    }
   ],
   "source": [
    "# Exploring starts\n",
    "episodes = exploring_starts(num_episodes=500)\n",
    "\n",
    "# Rewards per day\n",
    "rewards_per_day = [sum(reward for _, _, reward in episode) for episode in episodes]\n",
    "avg_reward = sum(rewards_per_day) / len(rewards_per_day)\n",
    "print(f\"Average reward per day (exploring starts): {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c02838ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per day (exploring starts): 339.808\n"
     ]
    }
   ],
   "source": [
    "# Soft Policy\n",
    "Q = {}\n",
    "epsilon_greedy = epsilon_greedy_policy(Q, epsilon)\n",
    "episodes = []\n",
    "for _ in range(500):\n",
    "    episodes.append(generate_episode(policy=epsilon_greedy, days=30))\n",
    "# Rewards per day\n",
    "rewards_per_day = [sum(reward for _, _, reward in episode) for episode in episodes]\n",
    "avg_reward = sum(rewards_per_day) / len(rewards_per_day)\n",
    "print(f\"Average reward per day (exploring starts): {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "c41b1cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per day (off-policy): 5987.066666666667\n"
     ]
    }
   ],
   "source": [
    "# Off-policy\n",
    "Q, rewards = off_policy_mc_control(num_episodes=500, days=30)\n",
    "\n",
    "# Rewards per day\n",
    "avg_reward = sum(rewards) / len(rewards)\n",
    "print(f\"Average reward per day (off-policy): {avg_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c733a91",
   "metadata": {},
   "source": [
    "### Preguntas\n",
    "1. ¿Cuál es el valor estimado de mantener diferentes niveles de existencias para cada producto?\n",
    "El valor estimado de mantener niveles distintos de stock es el encontrar un balance entre evitar pérdidas de ventas (no tener producto disponible) y minimizar costos de restock.\n",
    "2. ¿Cómo afecta el valor epsilon en la política blanda al rendimiento?\n",
    "Afecta en el momento de la exploración. Depende del ambiente que se trate si esto puede ser positivo, negativo o no tener algún efecto, ya que está la posibilidad de que al explorar mucho se encuentren mejores políticas, se encuentren peores, o no sume ni reste.\n",
    "3. ¿Cuál es el impacto de utilizar el aprendizaje fuera de la política en comparación con el aprendizaje dentro\n",
    "de la política?\n",
    "Usar un aprendizaje off-policy permite al agente aprender una mejor política mientras explora, lo que aprovecha mejor los datos. Es menos prudente que un agente on-policy ya que solo busca maximizar su reward sin limites establecidos. En el ambiente en uso (inventario de ventas), off-policy puede ser una mejor decisión ya que encontrará más rápido cómo maximizar las ganancias netas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
